{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 17470,
     "status": "ok",
     "timestamp": 1603706549445,
     "user": {
      "displayName": "Piera Carugno",
      "photoUrl": "",
      "userId": "12160015843414474967"
     },
     "user_tz": 0
    },
    "id": "pLV6CvNeJh1A",
    "outputId": "f67d35dd-2af3-4598-a674-5fa660d3225a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twint\n",
      "  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to c:\\users\\piera carugno\\appdata\\local\\temp\\pip-install-hszrv4uw\\twint\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (3.6.3)\n",
      "Requirement already satisfied, skipping upgrade: aiodns in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from twint) (4.9.1)\n",
      "Requirement already satisfied, skipping upgrade: cchardet in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (2.1.6)\n",
      "Requirement already satisfied, skipping upgrade: elasticsearch in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (7.9.1)\n",
      "Requirement already satisfied, skipping upgrade: pysocks in c:\\programdata\\anaconda3\\lib\\site-packages (from twint) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from twint) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp_socks in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (0.5.5)\n",
      "Requirement already satisfied, skipping upgrade: schedule in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: geopy in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: fake-useragent in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (0.1.11)\n",
      "Requirement already satisfied, skipping upgrade: googletransx in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from twint) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->twint) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: yarl<1.6.0,>=1.0 in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from aiohttp->twint) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from aiohttp->twint) (4.7.6)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from aiohttp->twint) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->twint) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pycares>=3.0.0 in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from aiodns->twint) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->twint) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from elasticsearch->twint) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from elasticsearch->twint) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->twint) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->twint) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->twint) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-socks[asyncio]>=1.0.1 in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from aiohttp_socks->twint) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: geographiclib<2,>=1.49 in c:\\users\\piera carugno\\appdata\\roaming\\python\\python38\\site-packages (from geopy->twint) (1.50)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from googletransx->twint) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from yarl<1.6.0,>=1.0->aiohttp->twint) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pycares>=3.0.0->aiodns->twint) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->twint) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.20)\n",
      "Building wheels for collected packages: twint\n",
      "  Building wheel for twint (setup.py): started\n",
      "  Building wheel for twint (setup.py): finished with status 'done'\n",
      "  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=39728 sha256=8c7b3bb3ab387e1fbe7f20dffa15a539e8c34b75324ec06f97f86fa89310638a\n",
      "  Stored in directory: C:\\Users\\Piera Carugno\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ie48v_i2\\wheels\\82\\67\\14\\5495652787d3c55288164de4234c0712a1f785a64f61f1ea70\n",
      "Successfully built twint\n",
      "Installing collected packages: twint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/twintproject/twint.git 'C:\\Users\\Piera Carugno\\AppData\\Local\\Temp\\pip-install-hszrv4uw\\twint'\n",
      "  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\n",
      "  Running command git checkout -q origin/master\n",
      "  WARNING: The script twint.exe is installed in 'C:\\Users\\Piera Carugno\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: twint\n",
      "    Found existing installation: twint 2.1.21\n",
      "    Uninstalling twint-2.1.21:\n",
      "      Successfully uninstalled twint-2.1.21\n",
      "Successfully installed twint-2.1.21\n"
     ]
    }
   ],
   "source": [
    "# pip install twint module from its repo\n",
    "!pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 3582,
     "status": "ok",
     "timestamp": 1603706691868,
     "user": {
      "displayName": "Piera Carugno",
      "photoUrl": "",
      "userId": "12160015843414474967"
     },
     "user_tz": 0
    },
    "id": "y4Mj5wLJ442_",
    "outputId": "fef576f8-b875-4a81-c289-9d5b412d3687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "## There are known issues running twint in jupyter, nest_asyncio may be able to solve the problem. \n",
    "## When in an environment where the event loop is already running it’s impossible to run tasks and wait for the result.\n",
    "## nest_asyncio may solve this problem\n",
    " \n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 23256,
     "status": "ok",
     "timestamp": 1603706720301,
     "user": {
      "displayName": "Piera Carugno",
      "photoUrl": "",
      "userId": "12160015843414474967"
     },
     "user_tz": 0
    },
    "id": "zuy9Sr2O5CuW",
    "outputId": "04d1fdd6-7664-4f9e-f406-6f2689500adb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-95e3e979e2bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Otherwise, anything that is saved within Google Colab environment will be deleted at the end of each session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# If working on google.colab, these code would be useful for mounting to your own google drive. \n",
    "# Otherwise, anything that is saved within Google Colab environment will be deleted at the end of each session\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2831,
     "status": "ok",
     "timestamp": 1603706747241,
     "user": {
      "displayName": "Piera Carugno",
      "photoUrl": "",
      "userId": "12160015843414474967"
     },
     "user_tz": 0
    },
    "id": "7HXpn38U5E2_"
   },
   "outputs": [],
   "source": [
    "#scientific and machine learning libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#plotting options\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as ms\n",
    "#% matplotlib inline\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "import seaborn as sns\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "#twint\n",
    "import twint\n",
    "#to avoid problems with running twint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3703,
     "status": "ok",
     "timestamp": 1603706769658,
     "user": {
      "displayName": "Piera Carugno",
      "photoUrl": "",
      "userId": "12160015843414474967"
     },
     "user_tz": 0
    },
    "id": "B-vofqyc5L0F"
   },
   "outputs": [],
   "source": [
    "## Sources: \n",
    "## https://www.ein.org.uk/news/migration-observatory-most-common-word-used-describe-immigrants-british-press-illegal\n",
    "## https://migrationobservatory.ox.ac.uk/resources/reports/decade-immigration-british-press/\n",
    "\n",
    "# basic list of terms to identify migrants\n",
    "imm_basic = [\"UK immigrant\", \"UK asylum seeker\", \"UK refugee\", \"UK borders\", \"UK illegal immigration\", \"UK undocumented migrant\"]\n",
    "\n",
    "###### Top ten modifiers of ‘immigrants’ or ‘migrants’, all publications, 2006-May 2015\n",
    "immigration_modifiers = [\"UK illegal\", \"UK EU\", \"UK European\", \"UK many\", \"UK new\", \"UK more\", \"UK African\", \"UK Jewish\", \"UK Polish\", \"UK Irish\", \"UK recent\"]\n",
    "\n",
    "immigrant = \" immigrant\"\n",
    "# list modifier to inlcude singular and plural (immigrant vs immigrantS)\n",
    "immigrant_mod = [s + immigrant + \" OR \" + s + immigrant + \"s\" for s in immigration_modifiers]\n",
    "\n",
    "migrant = \" migrant\"\n",
    "# list modifier to inlcude singular and plural (migrant vs migrantS)\n",
    "migrant_mod = [s + migrant + \" OR \" + s + migrant + \"s\" for s in immigration_modifiers]\n",
    "\n",
    "###### Top ten modifiers of ‘refugee(s)’, all publications, 2006-May 2015\n",
    "refugee_modifiers = [\"UK Syrian\", \"UK Palestinian\", \"UK Jewish\", \"UK many\", \"UK political\", \"UK Iraqi\", \"UK other\", \"UK more\", \"UK Afghan\", \"UK young\"]\n",
    "refugee = \" refugee\"\n",
    "# list modifier to inlcude singular and plural (refugee vs refugeeS)\n",
    "refugee_mod = [s + refugee + \" OR \" + s + refugee + \"s\" for s in refugee_modifiers]\n",
    "\n",
    "search_concatenation = imm_basic +  immigrant_mod + migrant_mod  + refugee_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-MxUEQxVWC86",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "No more data. finished scraping!! is it though? because sometimes twitter lie.\n",
      "225735\n"
     ]
    }
   ],
   "source": [
    "# the concept is to perform the scraping using the elements in the lists above: imm_basic, immigrant_mod, migrant_mod and refugee_mod\n",
    "appended_data = []\n",
    "for s in search_concatenation:\n",
    "    config = twint.Config()\n",
    "    config.Search = s\n",
    "    config.Lang = \"en\"\n",
    "    config.Limit = 500000\n",
    "    config.Since = \"2015-10-01 00:00:00\"\n",
    "    config.Until = \"2016-10-01 00:00:00\"\n",
    "    config.Hide_output = True\n",
    "    config.Pandas = True\n",
    "    twint.run.Search(config)\n",
    "    Tweets_df = twint.storage.panda.Tweets_df\n",
    "    appended_data.append(Tweets_df)  \n",
    "    \n",
    "# see pd.concat documentation for more info\n",
    "appended_data = pd.concat(appended_data).sort_values(by=['date']).reset_index()\n",
    "\n",
    "print(len(appended_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225735, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appended_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38, 8), 38)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with this command I'm checking \n",
    "\n",
    "appended_data.groupby('search').sum().shape, len(search_concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data.to_csv('../data/imm_df_2015_2016.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Twint_scraping_immigration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
